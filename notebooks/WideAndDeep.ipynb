{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep\n",
    "論文リンク：https://arxiv.org/pdf/1606.07792.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 17:07:15.590188 4583359936 deprecation_wrapper.py:119] From /Users/fumiyo_ito/Documents/git/Wide-Deep/notebooks/utils.py:7: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from configparser import ConfigParser\n",
    "from time import time, gmtime, strftime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import EarlyStoppingHook, export_result\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_bucket_size 8000\n",
      "item_bucket_size 12000\n",
      "wide_feature_dim 200000\n",
      "user_embedding_dim 258\n",
      "item_embedding_dim 258\n",
      "early_stop 2000\n",
      "dropout_prob 0.3\n"
     ]
    }
   ],
   "source": [
    "# configファイルの読み込み\n",
    "config_filename = '/Users/fumiyo_ito/Documents/git/Wide-Deep/notebooks/config/config_sample.ini'\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read(config_filename)\n",
    "\n",
    "for key in config['model'].keys():\n",
    "    print(key, config['model'][key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fumiyo_ito/Documents/git/Wide-Deep/data/train.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['filename_pattern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラムの定義\n",
    "HEADER = ['user_id', 'item_id', 'rating']\n",
    "HEADER_DEFAULTS = [['0'], ['0'], ['0']]\n",
    "\n",
    "FEATURE_NAMES = ['user_id', 'item_id']\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE = {\n",
    "  'user_id': int(config['model']['user_bucket_size']),\n",
    "  'item_id' : int(config['model']['item_bucket_size'])\n",
    "  }\n",
    "\n",
    "USED_FEATURE_NAMES = ['user_id', 'item_id', 'rating']\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES =  list(CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE.keys())\n",
    "TARGET = 'rating'\n",
    "TARGET_LABELS = ['0','1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_config(config, phase):\n",
    "    '''iniファイルをパースする関数\n",
    "    configparserで数値を引っ張るとstrになってしまうためここでintに変換している。\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    config: dict\n",
    "        phaseごとの設定を記した辞書\n",
    "    phase: str\n",
    "        学習のフェーズ\n",
    "        {'train', 'eval', 'predict'}のいずれか1つを指定\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "    filename_pattern: string\n",
    "    batch_size: int\n",
    "        学習時のバッチサイズ\n",
    "    num_epochs: int\n",
    "        学習で回す最大のepoch数\n",
    "    skip_header_lines: int\n",
    "        csvファイルのうち読み飛ばす行数\n",
    "    '''\n",
    "    filename_pattern = config[phase]['filename_pattern']\n",
    "    batch_size = int(config[phase]['batch_size'])\n",
    "    num_epochs = int(config[phase]['num_epochs']) # Noneにすると評価したが最後一生返ってこない\n",
    "    skip_header_lines = int(config[phase]['skip_header_lines'])\n",
    "\n",
    "    return filename_pattern, batch_size, num_epochs, skip_header_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    '''csvをparseする関数\n",
    "    csv_input_fn内で使用\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    csv_row: Tensor\n",
    "        string型の入力\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "    features, target: Tensorのlist\n",
    "        record_defaultsに指定された型と同じデータ型が期待される\n",
    "    '''\n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)\n",
    "    features = dict(zip(HEADER, columns))\n",
    "\n",
    "    target = features.pop(TARGET)\n",
    "    return features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(config, phase, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    '''csvからfeaturesとtargetを出力するinput_fnを返す関数\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    config: dict\n",
    "        phaseごとの設定を記した辞書\n",
    "    phase: str\n",
    "        学習のフェーズ\n",
    "        {'train', 'eval', 'predict'}のいずれか1つを指定\n",
    "    mode: tf.estimator.ModeKeys\n",
    "        学習のフェーズとほぼ同値（諸々の事情でphaseと分けている）\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "    features, target: Iterator\n",
    "        バッチの大きさだけ特徴量・目的変数を返すイテレータ\n",
    "    '''\n",
    "    filename_pattern, batch_size, num_epochs, skip_header_lines = parse_input_config(config, phase)\n",
    "\n",
    "    # ファイル名のパターンを元にデータの読み込み\n",
    "    file_names = tf.matching_files(filename_pattern)\n",
    "    dataset = tf.data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(int(config[phase]['batch_size']) * 2,\n",
    "                                  seed=0,\n",
    "                                  reshuffle_each_iteration=True)\n",
    "\n",
    "    # バッチサイズ分だけ切り出しgenerateする\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features, target = iterator.get_next()\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serving_input_fn():\n",
    "    '''serving用のinput_fn\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "    tf.estimator.export.ServingInputReceiver: Tensor\n",
    "    '''\n",
    "    receiver_tensor = {}\n",
    "    for feature_name in USED_FEATURE_NAMES:\n",
    "        dtype = tf.float32 if feature_name == TARGET else tf.string\n",
    "        receiver_tensor[feature_name] = tf.placeholder(shape=[None], dtype=dtype)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(receiver_tensor, receiver_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = lambda: csv_input_fn(config=config, \n",
    "                                      phase='train', \n",
    "                                      mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "eval_input_fn = lambda: csv_input_fn(config=config,\n",
    "                                     phase='eval', \n",
    "                                     mode=tf.estimator.ModeKeys.EVAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Early Stopping Hook: - Created\n",
      "*** Early Stopping Hook:: Early Stopping Rounds: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,\n",
    "                  max_steps=int(config['train']['max_steps']),\n",
    "                  hooks=[EarlyStoppingHook(int(config['model']['early_stop']))]\n",
    "                  )\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,\n",
    "                exporters=[tf.estimator.LatestExporter(name=\"estimate\",  \n",
    "                                                       serving_input_receiver_fn=json_serving_input_fn)],\n",
    "                steps=None,\n",
    "                throttle_secs = 15\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_execute_time = gmtime()\n",
    "execute_time = strftime(\"%Y%m%d_%H%M%S\", raw_execute_time )\n",
    "model_dir = os.path.join(config['path']['model_dir'], execute_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig().replace(model_dir=model_dir, save_checkpoints_secs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_feature_dim = int(config['model']['wide_feature_dim'])\n",
    "user_embedding_dim = int(config['model']['user_embedding_dim'])\n",
    "item_embedding_dim = int(config['model']['item_embedding_dim'])\n",
    "\n",
    "categorical_hash_user = \\\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('user_id', CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE['user_id'])\n",
    "categorical_hash_item = \\\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('item_id', CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE['item_id'])\n",
    "\n",
    "categorical_feature_user_x_categorical_feature_item = tf.feature_column.crossed_column(['user_id', 'item_id'], wide_feature_dim)\n",
    "categorical_feature_user_emb = tf.feature_column.embedding_column(\n",
    "    categorical_column=categorical_hash_user, dimension=user_embedding_dim)\n",
    "categorical_feature_item_emb = tf.feature_column.embedding_column(\n",
    "    categorical_column=categorical_hash_item, dimension=item_embedding_dim)\n",
    "\n",
    "wide_feature_columns = [categorical_feature_user_x_categorical_feature_item]\n",
    "deep_feature_columns = [categorical_feature_user_emb, categorical_feature_item_emb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_prob = float(config['model']['dropout_prob'])\n",
    "hidden_units = [128, 64, 32]\n",
    "\n",
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "                        n_classes=len(TARGET_LABELS),\n",
    "                        label_vocabulary=TARGET_LABELS,\n",
    "                        dnn_feature_columns=deep_feature_columns,\n",
    "                        linear_feature_columns=wide_feature_columns,\n",
    "                        dnn_hidden_units=hidden_units,\n",
    "                        dnn_optimizer=tf.train.AdamOptimizer(),\n",
    "                        dnn_activation_fn=tf.nn.relu,\n",
    "                        dnn_dropout=dropout_prob,\n",
    "                        model_dir=model_dir,\n",
    "                        config= run_config\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 17:07:38.694089 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0809 17:07:38.792532 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0809 17:07:38.815971 4583359936 deprecation.py:323] From <ipython-input-7-b68e763007ed>:36: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0809 17:07:38.853646 4583359936 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0809 17:07:39.038224 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3038: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W0809 17:07:40.080089 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0809 17:07:43.339173 4583359936 deprecation_wrapper.py:119] From /Users/fumiyo_ito/Documents/git/Wide-Deep/notebooks/utils.py:24: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0809 17:07:43.341150 4583359936 deprecation_wrapper.py:119] From /Users/fumiyo_ito/Documents/git/Wide-Deep/notebooks/utils.py:25: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
      "\n",
      "W0809 17:07:44.906400 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:2027: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0809 17:07:45.767141 4583359936 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0809 17:07:45.794663 4583359936 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0809 17:07:46.158799 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0809 17:07:47.821703 4583359936 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0809 17:07:47.823063 4583359936 export_utils.py:182] Export includes no default signature!\n",
      "W0809 17:07:48.239032 4583359936 estimator.py:1487] Training with estimator made no steps. Perhaps input is empty or misspecified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.0,\n",
       "  'accuracy_baseline': 1.0,\n",
       "  'auc': 0.0,\n",
       "  'auc_precision_recall': 0.0,\n",
       "  'average_loss': 0.0,\n",
       "  'label/mean': 0.0,\n",
       "  'loss': 0.0,\n",
       "  'precision': 0.0,\n",
       "  'prediction/mean': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'global_step': 0},\n",
       " [b'/Users/fumiyo_ito/Documents/git/Wide-Deep/models/20190809_080731/export/estimate/1565338067'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測&評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(config['predict']['filename_pattern'])\n",
    "test_size = len(test_data)\n",
    "\n",
    "predict_input_fn = lambda: csv_input_fn(config=config, \n",
    "                                      phase='predict', \n",
    "                                      mode=tf.estimator.ModeKeys.PREDICT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "values = list(map(lambda item: item[\"logistic\"][0],list(itertools.islice(predictions, test_size))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = np.array(test_data.iloc[:,2])\n",
    "pred_value = np.array(values)\n",
    "pred_value_binary = np.round(pred_value)\n",
    "\n",
    "auc = roc_auc_score(test_value, pred_value)\n",
    "accuracy = accuracy_score(test_value, pred_value_binary)\n",
    "print('AUC: {:.4f}\\nAccuracy: {:.4f}'.format(auc, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の出力\n",
    "model_name = 'WideAndDeep'\n",
    "export_result(model_name, auc, accuracy, config_filename, execute_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
